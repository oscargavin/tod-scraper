# Which.com Product Scraper

A high-performance, parallel web scraper for Which.com product reviews with intelligent retailer enrichment. Extracts product listings, detailed specifications, and enriches data with retailer-specific information from multiple sources.

## Features

- ðŸš€ **Fast Parallel Processing**: Multi-worker architecture for concurrent specification extraction
- ðŸ“Š **Complete Data Extraction**: Products, prices, specifications, features, and retailer links
- ðŸŽ¯ **Smart Pagination**: Automatically detects and processes all pages
- ðŸ›ï¸ **Retailer Enrichment**: Intelligently enriches products with specs from AO.com and other retailers
- ðŸ—ï¸ **Extensible Architecture**: Plugin-style retailer scrapers with priority-based selection
- ðŸ”„ **Single Browser Instance**: Efficient resource usage across entire pipeline
- ðŸ“ˆ **Progress Tracking**: Real-time updates showing worker progress
- ðŸ–¼ï¸ **Image Download**: Optional product image extraction and Supabase storage
- ðŸ› ï¸ **Flexible Options**: Skip specs/retailers extraction for quick product discovery

## Installation

1. **Clone the repository**:
```bash
cd /path/to/tod-scraper
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Install Playwright browsers**:
```bash
playwright install chromium
```

4. **(Optional) Set up Supabase** for image storage:
```bash
# Create .env file
echo "SUPABASE_URL=your_supabase_url" >> .env
echo "SUPABASE_KEY=your_supabase_key" >> .env
```

## Usage

### Basic Usage

Scrape all products from a category with specifications:
```bash
python src/scrapers/which/complete_scraper.py --url "https://www.which.co.uk/reviews/air-fryers" --pages all
```

### With Retailer Enrichment

Scrape and enrich with retailer specifications (e.g., AO.com):
```bash
python src/scrapers/which/complete_scraper.py \
  --url "https://www.which.co.uk/reviews/washing-machines" \
  --pages all \
  --enrich-retailers
```

### Full Pipeline (Recommended)

Run the complete pipeline with Which.com + AO reviews + Database insertion:
```bash
python scripts/run_full_pipeline.py \
  --url "https://www.which.co.uk/reviews/washing-machines" \
  --pages all \
  --insert-db \
  --generate-metadata
```

### Command Line Options

| Option | Short | Default | Description |
|--------|-------|---------|-------------|
| `--url` | `-u` | washing-machines | Which.com category URL to scrape |
| `--pages` | `-p` | 1 | Number of pages (`1-99` or `all`) |
| `--workers` | `-w` | 3 | Parallel workers for spec extraction (1-10) |
| `--skip-specs` | `-s` | False | Only get products, skip specifications |
| `--skip-retailers` | `-r` | False | Skip retailer link extraction |
| `--enrich-retailers` | `-e` | False | Enrich with retailer specifications |
| `--retailer-workers` | | 3 | Parallel workers for retailer enrichment (1-10) |
| `--download-images` | `-d` | False | Download and upload images to Supabase |
| `--output` | `-o` | complete_products.json | Output filename |

### Examples

**Full pipeline with retailer enrichment**:
```bash
python src/scrapers/which/complete_scraper.py \
  --url "https://www.which.co.uk/reviews/washing-machines" \
  --pages all \
  --workers 5 \
  --enrich-retailers \
  --retailer-workers 3 \
  --output washing_machines_full.json
```

**Scrape all air fryers with 5 workers**:
```bash
python src/scrapers/which/complete_scraper.py \
  --url "https://www.which.co.uk/reviews/air-fryers" \
  --pages all \
  --workers 5 \
  --output air_fryers.json
```

**Get first 3 pages of TVs (products only)**:
```bash
python src/scrapers/which/complete_scraper.py \
  --url "https://www.which.co.uk/reviews/tvs" \
  --pages 3 \
  --skip-specs \
  --output tvs_quick.json
```

**With image download to Supabase**:
```bash
python src/scrapers/which/complete_scraper.py \
  --url "https://www.which.co.uk/reviews/coffee-machines" \
  --pages 2 \
  --download-images
```

## Supported Categories

The scraper works with any Which.com reviews URL:

- **Kitchen Appliances**: air-fryers, coffee-machines, food-processors, microwaves
- **Large Appliances**: washing-machines, dishwashers, fridge-freezers, tumble-dryers
- **Electronics**: tvs, laptops, tablets, headphones, soundbars
- **Home & Garden**: vacuum-cleaners, lawn-mowers, pressure-washers
- **And many more...**

## Output Format

The scraper generates a JSON file with the following structure:

```json
{
  "products": [
    {
      "name": "Siemens IQ-500 i-Dos WG46H2A9GB",
      "price": 599.0,
      "whichUrl": "https://www.which.co.uk/reviews/...",
      "retailerLinks": [
        {
          "name": "AO",
          "price": "Â£599",
          "url": "https://ao.com/product/..."
        }
      ],
      "specs": {
        "type": "Freestanding",
        "height": "84.5cm",
        "wash_load": "9 Kg",
        "max_spin_speed": "1600 RPM",
        "energy_rating": "A",
        ...
      },
      "features": {
        "quick_wash": "Yes",
        "steam_function": "No",
        "smart_control": "Yes",
        ...
      },
      "retailerEnrichmentUrl": "https://ao.com/product/...",
      "retailerEnrichmentSource": "AO"
    }
  ],
  "total": 20,
  "url": "https://www.which.co.uk/reviews/washing-machines",
  "successful_enriched": 20,
  "failed_enriched": 0,
  "total_specs_extracted": 927,
  "total_features_extracted": 200
}
```

## Retailer Enrichment Architecture

The scraper features an extensible plugin architecture for enriching product data from multiple retailers.

### How It Works

1. **Automatic Retailer Detection**: Identifies available retailers from Which.com's "Where to buy" links
2. **Priority-Based Selection**: Tries retailers in configured priority order
3. **Intelligent Fallback**: Falls back to next retailer if primary fails or has insufficient data
4. **Quality Scoring**: Tracks data coverage and selects best source
5. **Spec Merging**: Intelligently merges retailer specs with Which.com specs

### Current Retailers

- âœ… **AO.com**: Full specification extraction (49-73 specs per product)
- âœ… **Appliance Centre**: Full specification extraction (50+ specs per product)
- ðŸ”œ **Marks Electrical**: Coming soon
- ðŸ”œ **Currys**: Coming soon
- ðŸ”œ **John Lewis**: Coming soon

### Configuration

Retailer behavior is controlled via `config/retailer_config.json`:

```json
{
  "priority_order": ["AO", "Marks Electrical", "Currys"],
  "fallback_enabled": true,
  "max_fallback_attempts": 2,
  "min_specs_threshold": 20,
  "stop_at_first_success": true,
  "scrapers": {
    "AO": {
      "enabled": true,
      "expected_spec_count": 50
    }
  }
}
```

### Adding New Retailers

The architecture makes adding new retailers simple:

1. **Create scraper class** in `src/scrapers/retailers/your_retailer_scraper.py`:
```python
from src.scrapers.retailers.base import RetailerScraper

class YourRetailerScraper(RetailerScraper):
    @property
    def retailer_name(self) -> str:
        return "Your Retailer"

    @property
    def url_patterns(self) -> List[str]:
        return ['yourretailer.co.uk']

    async def scrape_product(self, page, url: str) -> Dict:
        # Your scraping logic here
        pass

    def clean_url(self, url: str) -> str:
        # URL cleaning logic
        pass
```

2. **Register in orchestrator** (`src/scrapers/retailers/orchestrator.py`):
```python
def _register_all_scrapers(self):
    self.registry.register(AOScraper())
    self.registry.register(ApplianceCentreScraper())
    self.registry.register(YourRetailerScraper())  # Add this line
```

3. **Enable in config** (`config/retailer_config.json`):
```json
{
  "scrapers": {
    "Your Retailer": {
      "enabled": true,
      "expected_spec_count": 40
    }
  }
}
```

That's it! The orchestrator handles everything else automatically.

## Performance

- **Product Discovery**: ~2-3 seconds per page
- **Specification Extraction**: ~2-4 seconds per product (parallelized)
- **Retailer Enrichment**: ~3-5 seconds per product with available retailer link
- **Example**: 20 washing machines with full enrichment in ~60 seconds with 3 workers

### Optimization Tips

1. **More Workers**: Increase `--workers` and `--retailer-workers` for faster processing
2. **Skip What You Don't Need**: Use `--skip-specs` or `--skip-retailers` to speed up
3. **Specific Pages**: Limit pages instead of using `all` for testing
4. **Parallel Retailers**: Set `"parallel_scraping": true` in config (experimental)

## How It Works

The scraper operates in three phases:

### Phase 1: Product Discovery
- Navigates to the Which.com category URL
- Detects total pages automatically
- Extracts product names, prices, and URLs
- Extracts retailer links from "Where to buy" sections
- Removes duplicates

### Phase 2: Which.com Specification Extraction
- Distributes products evenly among workers
- Each worker maintains a persistent browser context
- Navigates to individual product pages
- Extracts specifications and features tables
- Extracts retailer prices and links
- Optionally downloads and uploads product images
- Returns structured data

### Phase 3: Retailer Enrichment (Optional)
- Filters products with retailer links
- Orchestrator selects best retailer per product based on:
  - Availability in retailer links
  - Priority order from config
  - Historical success rate
- Parallel workers scrape retailer pages
- Extracts retailer-specific specifications
- Merges retailer specs with Which.com specs
- Tracks enrichment source and quality

## Project Structure

```
tod-scraper/
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ scrapers/                  # Web scraping modules
â”‚   â”‚   â”œâ”€â”€ which/                 # Which.com scrapers
â”‚   â”‚   â”‚   â”œâ”€â”€ complete_scraper.py   # Main Which.com scraper
â”‚   â”‚   â”‚   â””â”€â”€ batch_scraper.py      # Batch category processor
â”‚   â”‚   â””â”€â”€ retailers/             # Retailer-specific scrapers
â”‚   â”‚       â”œâ”€â”€ base.py               # Abstract base class
â”‚   â”‚       â”œâ”€â”€ registry.py           # Scraper registration
â”‚   â”‚       â”œâ”€â”€ orchestrator.py       # Retailer selection & coordination
â”‚   â”‚       â”œâ”€â”€ ao_scraper.py         # AO.com scraper
â”‚   â”‚       â””â”€â”€ appliance_centre_scraper.py  # Appliance Centre scraper
â”‚   â”‚
â”‚   â”œâ”€â”€ reviews/                   # Review enrichment system
â”‚   â”‚   â””â”€â”€ ao/                    # AO.com review scrapers
â”‚   â”‚       â”œâ”€â”€ enricher.py           # Review enrichment pipeline
â”‚   â”‚       â”œâ”€â”€ search.py             # Product search
â”‚   â”‚       â”œâ”€â”€ sentiment_analyzer.py # Sentiment analysis
â”‚   â”‚       â””â”€â”€ sentiment_scraper.py  # Review scraping
â”‚   â”‚
â”‚   â”œâ”€â”€ database/                  # Database operations
â”‚   â”‚   â”œâ”€â”€ inserters/             # Data insertion scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ products.py           # Insert products
â”‚   â”‚   â”‚   â”œâ”€â”€ metadata.py           # Insert metadata
â”‚   â”‚   â”‚   â””â”€â”€ main_db.py            # Main DB insertion
â”‚   â”‚   â””â”€â”€ updaters/              # Data update scripts
â”‚   â”‚       â””â”€â”€ retailer_links.py     # Update retailer links
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # Utility functions
â”‚       â”œâ”€â”€ metadata_generator.py     # Generate product metadata
â”‚       â””â”€â”€ url_cleaner.py            # Clean tracking URLs
â”‚
â”œâ”€â”€ scripts/                       # Executable entry points
â”‚   â”œâ”€â”€ run_full_pipeline.py       # Full scraping pipeline
â”‚   â”œâ”€â”€ run_all_categories.py      # Batch category processor
â”‚   â””â”€â”€ scrape_ao_product.py       # AO product scraper
â”‚
â”œâ”€â”€ tests/                         # Test files
â”œâ”€â”€ config/                        # Configuration files
â”‚   â””â”€â”€ retailer_config.json       # Retailer settings
â”œâ”€â”€ output/                        # Generated JSON files
â”œâ”€â”€ docs/                          # Documentation
â””â”€â”€ migrations/                    # Database migrations
```

## Requirements

- Python 3.7+
- 4GB RAM minimum (8GB recommended for many workers)
- Stable internet connection
- Chrome/Chromium browser (installed via Playwright)

## Troubleshooting

### Common Issues

**Timeout errors**:
- Reduce number of workers
- Check internet connection
- Try again (retailers may have temporary issues)

**Missing specifications**:
- Some products may not have specs available
- Check the `specs_error` field in output
- Some retailer pages may be out of stock or unavailable

**No retailer enrichment**:
- Ensure `--enrich-retailers` flag is set
- Check that retailer is enabled in `config/retailer_config.json`
- Verify retailer links exist in Which.com data (don't use `--skip-retailers`)

**Memory issues**:
- Reduce workers to 3 or less
- Process fewer pages at once
- Disable image download if enabled

### Debug Mode

View browser actions (non-headless mode):
```python
# In src/scrapers/which/complete_scraper.py or src/scrapers/retailers/ao_scraper.py, change:
browser = await p.chromium.launch(
    headless=False,  # Change from True
    ...
)
```

## License

This tool is for educational purposes. Please respect Which.com's and retailers' terms of service and robots.txt.

## Contributing

Improvements welcome! Key areas:
- Additional retailer scrapers (Marks Electrical, Currys, John Lewis)
- Export formats (CSV, Excel)
- Retry logic for failed requests
- Price history tracking
- Review sentiment analysis integration
- Advanced scraping strategies for dynamic content

## Recent Updates

### December 2024 - Project Reorganization
- Restructured codebase into logical `src/` hierarchy
- Separated scrapers, reviews, database operations, and utilities
- Centralized configuration in `config/` directory
- Moved executable scripts to `scripts/` directory
- Consolidated all tests in `tests/` directory
- Added Appliance Centre retailer scraper
